{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXxah7U7uxdV7lX4s+Pj7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolai5965/MNIST_TensorFlow_From_Udemy_Course/blob/main/MNIST_TensorFlow_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing packages"
      ],
      "metadata": {
        "id": "0O8kOBYC7SNH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_2qWM-X7ApB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the MNIST dataset from TensorFlowÂ´s datasets"
      ],
      "metadata": {
        "id": "wKfi4gNl7SYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "w4d8eLrV7ZeX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing some pre processing of the data, and splitting the data up in training, validation and testing datasets. "
      ],
      "metadata": {
        "id": "WfJAqpaMrmCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.\n",
        "  return image, label\n",
        "\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "buffer_size = 10000\n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(buffer_size)\n",
        "\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_data = train_data.batch(batch_size)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "GGtwX6GN75ci"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the model"
      ],
      "metadata": {
        "id": "g_YxFHRmDbjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 200\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the optimizer, loss function and evaluation metric\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "metric = 'accuracy'\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[metric])\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "\n",
        "#model.fit(train_data, epochs=num_epochs, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "3k7cROEr9Qij"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks the time it take to fit the model. "
      ],
      "metadata": {
        "id": "1WpZdc1rtEwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "model.fit(train_data, epochs=num_epochs, validation_data=(validation_inputs, validation_targets), validation_steps=1, verbose=2)\n",
        "end = time.time()\n",
        "print(\"Total traning time took:\", end - start, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46aTFBkK9Seu",
        "outputId": "67319f6e-41a0-48dc-b909-a8d113b95e2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 14s - loss: 0.2571 - accuracy: 0.9223 - val_loss: 0.1327 - val_accuracy: 0.9612 - 14s/epoch - 26ms/step\n",
            "Epoch 2/5\n",
            "540/540 - 8s - loss: 0.1071 - accuracy: 0.9668 - val_loss: 0.0978 - val_accuracy: 0.9730 - 8s/epoch - 16ms/step\n",
            "Epoch 3/5\n",
            "540/540 - 6s - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.0728 - val_accuracy: 0.9768 - 6s/epoch - 12ms/step\n",
            "Epoch 4/5\n",
            "540/540 - 6s - loss: 0.0577 - accuracy: 0.9819 - val_loss: 0.0804 - val_accuracy: 0.9758 - 6s/epoch - 11ms/step\n",
            "Epoch 5/5\n",
            "540/540 - 6s - loss: 0.0505 - accuracy: 0.9844 - val_loss: 0.0517 - val_accuracy: 0.9850 - 6s/epoch - 11ms/step\n",
            "Total traning time took: 53.071879386901855 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got val_accuracy at 0.9850"
      ],
      "metadata": {
        "id": "40Pb6bmftOPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model:"
      ],
      "metadata": {
        "id": "4WV0QV5TgdLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy =  model.evaluate(test_data)\n",
        "print(\"Test loss: {:.2f}\".format(test_loss))\n",
        "print(\"Test accuracy: {:.2f}%\".format(100*test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlxmG1LdgyFQ",
        "outputId": "d0fe2b27-8030-4b92-e8f5-a26be1448052"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 0.0875 - accuracy: 0.9758\n",
            "Test loss: 0.09\n",
            "Test accuracy: 97.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got Test loss: 0.09\n",
        "and Test accuracy: 97.58%"
      ],
      "metadata": {
        "id": "bsyqWzn5tfLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data\n",
        "predictions = model.predict(test_data)\n",
        "# Print the shape of predictions\n",
        "print(\"Shape of predictions:\", predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTy1s8LMDd20",
        "outputId": "05a53c16-61a8-41bc-bcd4-acc3fc02bdc1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 863ms/step\n",
            "Shape of predictions: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''This code compiles and trains a neural network model using TensorFlow. \n",
        "The model is compiled with an Adam optimizer, SparseCategoricalCrossentropy loss function, and accuracy metric. \n",
        "The number of epochs for training is set to 5.\n",
        "The model is then trained on the training data and validated on the validation data. \n",
        "The total training time is printed after training is complete. The model is evaluated on the'''"
      ],
      "metadata": {
        "id": "FF6O1BDjn9ih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}